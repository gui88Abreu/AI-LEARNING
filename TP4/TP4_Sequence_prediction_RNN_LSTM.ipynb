{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"TP4_Sequence_prediction_RNN_LSTM.ipynb","provenance":[{"file_id":"1RT7xAUPQUwgPwPapMKCmPgIdAhl76ISx","timestamp":1585862270398}],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"x9VUagMroNkj","colab_type":"text"},"source":["# **Machine Learning Lab 4EII - IA course**\n","***Lab 4 - Sequence of words prediction with RNN and LSTM***"]},{"cell_type":"markdown","metadata":{"id":"RdNqp46loNkm","colab_type":"text"},"source":["## 1- **Introduction**\n","\n","This lab aim at using variants of **recurrent neural networks** for sequence of words semantic prediction. The variants of RNN can be used in combination with dense dense layers or convolutional neural networks for temporal regression and classification problems.  \n","\n","In this lab you will learn to:\n","* Build you variants of RNN Networks: GRU, LSTM.\n","* Apply these networks on semantic words sequence prediction. \n","* Build a network combining  CNN with LSTM  for semantic words sequence prediction\n","* LSTM network for translating short English sentence into short French sentence"]},{"cell_type":"markdown","metadata":{"id":"kb0Ckwt51NSu","colab_type":"text"},"source":["## 2- **Module importation**\n","Import some useful and common python modules "]},{"cell_type":"code","metadata":{"id":"5hxU_LEZoNko","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import  Dense, Embedding, Flatten, Convolution1D, Dropout, Input, MaxPooling1D, Activation, SimpleRNN, LSTM                                                                                                                                                                                        "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Oc7KsfZWoNks","colab_type":"text"},"source":["## 3- **Download and study the the IMDB dataset**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LHOn5QhATNph","colab_type":"text"},"source":["### 3.a - IMDB Dataset of 50K Movie Reviews\n","\n","> The IMDB sentiment classification dataset consists of 50,000 movie reviews from IMDB users that are labeled as either positive (1) or negative (0). The reviews are preprocessed and each one is encoded as a sequence of word indexes in the form of integers. The words within the reviews are indexed by their overall frequency within the dataset. For example, the integer “2” encodes the second most frequent word in the data. The 50,000 reviews are split into 25,000 for training and 25,000 for testing.\n","\n","> The dataset was created by researchers at Stanford University and published in a 2011 [paper](https://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf), where they achieved 88.89% accuracy. We try in this Lab to reach this performance. \n","\n","> The following IMDBdataset class provides methods to download the IMDB dataset and show its features.   "]},{"cell_type":"code","metadata":{"id":"jcNOgG4E4z3Z","colab_type":"code","colab":{}},"source":["class IMDBdataset:    \n","  def __init__(self, vocabulary_size=0):  \n","    self.vocabulary_size = vocabulary_size; \n","    \n","    self.X_train = []; \n","    self.X_test = []; \n","    self.y_train = []; \n","    self.y_test = []; \n","    self.data = []; \n","    self.targets = []; \n","  \n","  def loadData(self):\n","    from tensorflow.keras.datasets import imdb;\n","    (self.X_train, self.y_train), (self.X_test, self.y_test) = imdb.load_data(num_words=vocabulary_size);\n","    self.data = np.concatenate((self.X_train, self.X_test), axis=0)\n","    self.targets = np.concatenate((self.y_train, self.y_test), axis=0)\n","  \n","  def paddingData(self, max_review_length):\n","    from tensorflow.keras.preprocessing import sequence\n","    self.X_train = sequence.pad_sequences(self.X_train, maxlen=max_review_length)\n","    self.X_test = sequence.pad_sequences(self.X_test, maxlen=max_review_length)\n","  \n","  def printInfo(self):\n","    print('Size of the trainig set ' + str(len(self.X_train)))\n","    print('Size of the testing set ' + str(len(self.X_test)))\n","\n","    print(\"Categories:\", np.unique(self.targets), ' <==> ', '[negative, positive]')\n","    print(\"Number of unique words:\", len(np.unique(np.hstack(self.data))))\n","    print(\"Max sentence length :\", np.max( [len(self.data[i]) for i in range(0, len(self.data)) ]) )\n","\n","    length = [len(i) for i in self.data]\n","    print(\"Average Review length: %.2f \" % np.mean(length))\n","    print(\"Standard Deviation:\", round(np.std(length)))\n","  \n","  def printSentence(self, Id=0):\n","    from tensorflow.keras.datasets import imdb;\n","    if(Id >= len(self.data)):\n","      Id = 0;\n","\n","    print(\"Label:\", self.targets[Id])\n","    print(\"Number of words : \", len(self.data[Id]))\n","    print('Coded sentence : ', self.data[Id])\n","\n","    index = imdb.get_word_index()\n","    reverse_index = dict([(value, key) for (key, value) in index.items()]) \n","    decoded = \" \".join( [reverse_index.get(i - 3, \"#\") for i in self.data[Id]] )\n","    print('Original sentence : ' + decoded) \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NfCUtp2PM-mt","colab_type":"text"},"source":["### 3.b - Download the IMDBdataset dataset and analys it \n","\n","> Create an instance of the IMDB dataset calss \n","\n","> Set the vocabulary_size to 10000\n","\n","> vocabulary_size is the number of different words used in the dataset  "]},{"cell_type":"code","metadata":{"id":"d-yKuyP1CdTn","colab_type":"code","colab":{}},"source":["vocabulary_size = 10000\n","# TO DO "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qw4gLsaEN9LR","colab_type":"text"},"source":["> download the dataset with loadData() method "]},{"cell_type":"code","metadata":{"id":"XEruKslyzqVi","colab_type":"code","colab":{}},"source":["# TO DO "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VkG16KJNPAYs","colab_type":"text"},"source":["> Print some information of the IMDB dataset: size of training and testing sets, classes, nomber of words, ...\n","\n","> Use printInfo() method  "]},{"cell_type":"code","metadata":{"id":"L2Z2pvR22Guk","colab_type":"code","colab":{}},"source":["# TO DO "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f8NSus-zP8O9","colab_type":"text"},"source":["> Use can use printSentence(Id) method to print sentence of index Id before (original sentence) and after Tokenization.\n","\n","> It should be noted that the sentences are first Tokenized: each word is represented by a digit in the interval [1, vocabulary_size-1] as pre-processing step. In this dataset the words are sorted by the most represented words which is represented by 1 and so on...\n","\n","    \n","\n"]},{"cell_type":"code","metadata":{"id":"7QF1zdvtEUQI","colab_type":"code","colab":{}},"source":["IdL = [1, 2, 8]\n","for sentenceId in IdL:\n","  # TO DO\n","  print(' -------- ')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8P5b_cAIIjcJ","colab_type":"text"},"source":["### 3.b - Padding the data set review to have the same size.\n","\n","> You have noticed that the sentences have not the same length.\n","\n","> The paddingData method is provide to add padding to get sentences of the same length  of max_review_length.\n"]},{"cell_type":"code","metadata":{"id":"Z7xSVMRZoNkt","colab_type":"code","colab":{}},"source":["max_review_length = 500\n","# TO DO "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UQnh_R2-hgGA","colab_type":"text"},"source":["## 4- **Neural network and CNN for IMDB classification problem**\n","\n","> In this section you will develop a simple NN and CNN models for IMDB classification problem\n"]},{"cell_type":"markdown","metadata":{"id":"gKuOq6ysdQzo","colab_type":"text"},"source":["### 4.a - Two layers NN for IMDB dataset classification \n","\n","> Complete the following code to create a NN model wiht two layers (hidden dense layer of size 16 with Relu activation function)"]},{"cell_type":"code","metadata":{"id":"5Dz7IOpTSMci","colab_type":"code","colab":{}},"source":["hidden_layer_dim = 16;\n","model = Sequential()\n","\n","# TO DO \n","\n","print(model.summary())\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Juu46IkdrGd","colab_type":"text"},"source":["> Train the model and assess its performance on the testing set. "]},{"cell_type":"code","metadata":{"id":"WQyihOsBOt1U","colab_type":"code","colab":{}},"source":["# Fit the model\n","logsNN1 = model.fit(dataset.X_train, dataset.y_train, validation_data=(dataset.X_test, dataset.y_test), epochs=10, batch_size=128, verbose=2)\n","# Final evaluation of the model\n","scores = model.evaluate(dataset.X_test, dataset.y_test, verbose=0)\n","print(\"Accuracy: %.2f%%\" % (scores[1]*100))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SnNiYXuHjZTx","colab_type":"code","colab":{}},"source":["plt.plot(logsNN1.history['loss'], label='Loss (testing data NN1)')\n","plt.plot(logsNN1.history['val_loss'], label='Loss (validation data NN1)')\n","\n","plt.title('Binary cross entropy loss function')\n","plt.ylabel('Loss Error')\n","plt.xlabel('No. epoch')\n","plt.legend(loc=\"upper center\")\n","plt.grid()\n","plt.show()\n","\n","\n","plt.plot(logsNN1.history['accuracy'], label='Accuracy (testing data NN1)')\n","plt.plot(logsNN1.history['val_accuracy'], label='Accuracy (validation data NN1)')\n","plt.title('Accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('No. epoch')\n","plt.legend(loc=\"upper left\")\n","plt.grid()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K_hFGBfUgvAC","colab_type":"text"},"source":["### 4.b - Add an embedding layer  \n","\n","> Perform a linear projection of the input from input dimention to more compact new dimention (embedding_dim)\n","\n","> Each single word will be project to a vector of dimension embedding_dim\n","\n","> Use Embedding layer to perform this projection followed by a falatten layer and the two dense layers (hidden layer of size 16 and output layer)\n","\n","> Embedding layer must specify 3 arguments:\n","\n","1. input_dim: this is the size of the vocabulary in the text data (vocabulary_size). \n","\n","2. output_dim: this is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. In this Lab, we consider  output_dim =20.\n","\n","3. input_length: this is the length of input sequences, as you would define for any input layer of a Keras model (max_review_length).\n","\n","> Further details on Embedding layer can be found [here](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/). \n"]},{"cell_type":"code","metadata":{"id":"u9TXwibVPQrH","colab_type":"code","colab":{}},"source":["embedding_dim=20\n","model = Sequential()\n","\n","# TO DO \n","\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","print(model.summary())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lP-yMky82EuB","colab_type":"code","colab":{}},"source":["# Fit the model\n","logsNN2 = model.fit(dataset.X_train, dataset.y_train, validation_data=(dataset.X_test, dataset.y_test), epochs=10, batch_size=128, verbose=2)\n","# Final evaluation of the model\n","scores = model.evaluate(dataset.X_test, dataset.y_test, verbose=0)\n","print(\"Accuracy: %.2f%%\" % (scores[1]*100))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iHX6GZADdLvn","colab_type":"code","colab":{}},"source":["plt.plot(logsNN2.history['loss'], label='Loss (testing data NN2)')\n","plt.plot(logsNN2.history['val_loss'], label='Loss (validation data NN2)')\n","\n","plt.title('Binary cross entropy loss function')\n","plt.ylabel('Loss Error')\n","plt.xlabel('No. epoch')\n","plt.legend(loc=\"center right\")\n","plt.grid()\n","plt.show()\n","\n","\n","plt.plot(logsNN2.history['accuracy'], label='Accuracy (testing data NN2)')\n","plt.plot(logsNN2.history['val_accuracy'], label='Accuracy (validation data NN2)')\n","plt.title('Accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('No. epoch')\n","plt.legend(loc=\"lower right\")\n","plt.grid()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1OAgWtOYxjd_","colab_type":"text"},"source":["### 4.c - Convolutional Neural Network  \n","\n","> In this section you will build a convolutional neural network with using one cov layer and one dense output layer.\n","\n","> The conv layer is composed of:\n","\n","1.   Convolution1D layer of 200 filters of kernel size 3n  padding='valid' activation='relu' \n","2.   Max pooling layer (MaxPooling1D(pool_size=max_review_length-kernel_size+1))\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"2O0_pVJN6Xov","colab_type":"code","colab":{}},"source":["dropout_prob = 0.4\n","graph_in = Input(shape=(max_review_length, embedding_dim))\n","kernel_size = 3;\n","nb_filters = 32;\n","model = Sequential()\n","\n","# TO DO Embedding \n","\n","model.add(Dropout(dropout_prob))\n","\n","# TO DO conv + max pooling  + output dense layer \n","\n","model.compile(loss='binary_crossentropy',\n","              optimizer='adam',\n","              metrics=['acc'])\n","print(model.summary())\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oOR5MJP4NFI2","colab_type":"code","colab":{}},"source":["# Fit the model\n","logsCNN1 = model.fit(dataset.X_train, dataset.y_train, validation_data=(dataset.X_test, dataset.y_test), epochs=10, batch_size=128, verbose=2)\n","# Final evaluation of the model\n","scores = model.evaluate(dataset.X_test, dataset.y_test, verbose=0)\n","print(\"Accuracy: %.2f%%\" % (scores[1]*100))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QhAQkqlZ1dYf","colab_type":"code","colab":{}},"source":["plt.plot(logsCNN1.history['loss'], label='Loss (testing data CNN)')\n","plt.plot(logsCNN1.history['val_loss'], label='Loss (validation data CNN)')\n","\n","plt.title('Binary cross entropy loss function')\n","plt.ylabel('Loss Error')\n","plt.xlabel('No. epoch')\n","plt.legend(loc=\"center right\")\n","plt.grid()\n","plt.show()\n","\n","plt.plot(logsCNN1.history['acc'], label='Accuracy (testing data CNN)')\n","plt.plot(logsCNN1.history['val_acc'], label='Accuracy (validation data CNN)')\n","plt.title('Accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('No. epoch')\n","plt.legend(loc=\"lower right\")\n","plt.grid()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ns1ua0rv0ik_","colab_type":"text"},"source":["## 5- **Recurrent Neural Network and LSTM for IMDB dataset sentiment classification problem**\n","\n","> Instead of 1D conv layer used in the previous section, here you will use a simple RNN and then a LSTM"]},{"cell_type":"markdown","metadata":{"id":"91SYud4eSGkR","colab_type":"text"},"source":["### 5.a - Build a model with a simple RNN layer SimpleRNN"]},{"cell_type":"markdown","metadata":{"id":"KdmsZquU0V7G","colab_type":"text"},"source":["> Use SimpleRNN layer with rnn_cells as parameter. "]},{"cell_type":"code","metadata":{"id":"laxEJvPWlcll","colab_type":"code","colab":{}},"source":["# Simple RNN\n","rnn_cells = 64; \n","model = Sequential()\n","\n","# TO DO \n","\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-rn1yjTqlwTs","colab_type":"code","colab":{}},"source":["# Fit the model\n","logsRNN = model.fit(dataset.X_train, dataset.y_train, validation_data=(dataset.X_test, dataset.y_test), epochs=10, batch_size=128, verbose=2)\n","# Final evaluation of the model\n","scores = model.evaluate(dataset.X_test, dataset.y_test, verbose=0)\n","print(\"Accuracy: %.2f%%\" % (scores[1]*100))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZQDtWCj737mY","colab_type":"code","colab":{}},"source":["plt.plot(logsRNN.history['loss'], label='Loss (testing data RNN)')\n","plt.plot(logsRNN.history['val_loss'], label='Loss (validation data RNN)')\n","\n","plt.title('Binary cross entropy loss function')\n","plt.ylabel('Loss Error')\n","plt.xlabel('No. epoch')\n","plt.legend(loc=\"center right\")\n","plt.grid()\n","plt.show()\n","\n","\n","plt.plot(logsRNN.history['accuracy'], label='Accuracy (testing data RNN)')\n","plt.plot(logsRNN.history['val_accuracy'], label='Accuracy (validation data RNN)')\n","plt.title('Accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('No. epoch')\n","plt.legend(loc=\"lower right\")\n","plt.grid()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JknqQvzUSRPC","colab_type":"text"},"source":["### 5.b - Build a model with a LSTM layer LSTM"]},{"cell_type":"code","metadata":{"id":"9H73mgUTnaSI","colab_type":"code","colab":{}},"source":["# LSTM \n","model = Sequential()\n","lstm_cells =64; \n","\n","# TO DO \n","\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WqLS1yQRlq08","colab_type":"code","colab":{}},"source":["# Fit the model\n","logsLSTM = model.fit(dataset.X_train, dataset.y_train, validation_data=(dataset.X_test, dataset.y_test), epochs=10, batch_size=128, verbose=2)\n","# Final evaluation of the model\n","scores = model.evaluate(dataset.X_test, dataset.y_test, verbose=0)\n","print(\"Accuracy: %.2f%%\" % (scores[1]*100))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FQbz3cNS0e2v","colab_type":"code","colab":{}},"source":["plt.plot(logsLSTM.history['loss'], label='Loss (testing data LSTM)')\n","plt.plot(logsLSTM.history['val_loss'], label='Loss (validation data LSTM)')\n","\n","plt.title('Binary cross entropy loss function')\n","plt.ylabel('Loss Error')\n","plt.xlabel('No. epoch')\n","plt.legend(loc=\"center right\")\n","plt.grid()\n","plt.show()\n","\n","\n","plt.plot(logsLSTM.history['accuracy'], label='Accuracy (testing data LSTM)')\n","plt.plot(logsLSTM.history['val_accuracy'], label='Accuracy (validation data LSTM)')\n","plt.title('Accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('No. epoch')\n","plt.legend(loc=\"lower right\")\n","plt.grid()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pnc5S6CE0Pi6","colab_type":"text"},"source":["> What are the advantages of using LSTM tan a simple RNN. "]},{"cell_type":"markdown","metadata":{"id":"XY1t-7YjSbtJ","colab_type":"text"},"source":["### 5.c - Build a model combiling conv layer with LSTM layer "]},{"cell_type":"code","metadata":{"id":"pXm73s7DrDFJ","colab_type":"code","colab":{}},"source":["# conv + LSTM \n","graph_in = Input(shape=(max_review_length, embedding_dim))\n","model = Sequential()\n","\n","# TO DO \n","\n","model.compile(loss='binary_crossentropy',\n","              optimizer='adam',\n","              metrics=['acc'])\n","print(model.summary())\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ptf309vhKya3","colab_type":"code","colab":{}},"source":["# Fit the model\n","logsCNNLSTM = model.fit(dataset.X_train, dataset.y_train, validation_data=(dataset.X_test, dataset.y_test), epochs=10, batch_size=128, verbose=2)\n","# Final evaluation of the model\n","scores = model.evaluate(dataset.X_test, dataset.y_test, verbose=0)\n","print(\"Accuracy: %.2f%%\" % (scores[1]*100))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tDNZjKAtfC0T","colab_type":"text"},"source":["> You can find [here](https://paperswithcode.com/sota/sentiment-analysis-on-imdb) a review on state of the art models for IMDB emotion classification. You can compare your performance with these models. \n"]},{"cell_type":"markdown","metadata":{"id":"lBuYO-ksS5WP","colab_type":"text"},"source":["## 6- **LSTM for translating short English sentences into short French sentences (Question bonus)**\n","\n","> In this section you will develop a model based on LSTM for English to French short sentence translation. "]},{"cell_type":"markdown","metadata":{"id":"F78cNLI1idYz","colab_type":"text"},"source":["## 6.a- Download and prepare the dataset\n","> In this section you will download and prepare the English to French short sentences dataset.  \n","\n","> The model is composed of one encoder (LSTM layer) and one decoder (LSTM layer + 1 dense output layer) \n","\n","> In this model you will be able to translate sentences of variable length. The states of the decoder  are initialized by the encoder state output, then the state of the decoder are updated with the decoder output and the decoded sample. \n","\n","> You can import the dataset file fra.txt in the working directory "]},{"cell_type":"code","metadata":{"id":"umjfRkH_gPNa","colab_type":"code","colab":{}},"source":["from __future__ import print_function\n","\n","from keras.models import Model\n","from keras.layers import Input, LSTM, Dense\n","import numpy as np\n","from google.colab import drive\n","\n","\n","\n","# Path to the data txt file on disk.\n","\n","num_samples = 10000  # Number of samples to train on.\n","\n","class datasetFrEg: \n","  def __init__(self, data_path):  \n","    self.data_path = data_path; \n","    \n","    self.X_train = []; \n","    self.X_test = []; \n","    self.y_train = []; \n","    self.y_test = []; \n","    self.data = []; \n","    self.targets = []; \n","  \n","    self.input_texts = []\n","    self.target_texts = []\n","    self.input_characters = set()\n","    self.target_characters = set()\n","\n","  def loadData(self):\n","    with open(self.data_path, 'r', encoding='utf-8') as f:\n","      lines = f.read().split('\\n')\n","    for line in lines[: min(num_samples, len(lines) - 1)]:\n","      input_text, target_text, s = line.split('\\t')\n","      # We use \"tab\" as the \"start sequence\" character\n","      # for the targets, and \"\\n\" as \"end sequence\" character.\n","      target_text = '\\t' + target_text + '\\n'\n","      self.input_texts.append(input_text)\n","      self.target_texts.append(target_text)\n","      for char in input_text:\n","        if char not in self.input_characters:\n","            self.input_characters.add(char)\n","      for char in target_text:\n","        if char not in self.target_characters:\n","            self.target_characters.add(char)\n","\n","  def printInfo(self):\n","    self.input_characters = sorted(list(self.input_characters))\n","    self.target_characters = sorted(list(self.target_characters))\n","    self.num_encoder_tokens = len(self.input_characters)\n","    self.num_decoder_tokens = len(self.target_characters)\n","    self.max_encoder_seq_length = max([len(txt) for txt in self.input_texts])\n","    self.max_decoder_seq_length = max([len(txt) for txt in self.target_texts])\n","\n","    print('Number of samples:', len(self.input_texts))\n","    print('Number of unique input tokens:', self.num_encoder_tokens)\n","    print('Number of unique output tokens:', self.num_decoder_tokens)\n","    print('Max sequence length for inputs:', self.max_encoder_seq_length)\n","    print('Max sequence length for outputs:', self.max_decoder_seq_length)\n","\n","  def printSample(self, Id):\n","    if Id < len(self.input_texts) : \n","      print(\"# setence En :\", self.input_texts[Id], \"# setence Fr :\", self.target_texts[Id])\n","\n","  def vectorizeData(self):\n","    self.input_token_index = dict([(char, i) for i, char in enumerate(self.input_characters)])\n","    self.target_token_index = dict([(char, i) for i, char in enumerate(self.target_characters)])\n","\n","    self.encoder_input_data = np.zeros((len(self.input_texts), self.max_encoder_seq_length, self.num_encoder_tokens), dtype='float32')\n","    self.decoder_input_data = np.zeros((len(self.input_texts), self.max_decoder_seq_length, self.num_decoder_tokens),dtype='float32')\n","    self.decoder_target_data = np.zeros((len(self.input_texts), self.max_decoder_seq_length, self.num_decoder_tokens),dtype='float32')\n","\n","    for i, (self.input_text, self.target_text) in enumerate(zip(self.input_texts, self.target_texts)):\n","      for t, char in enumerate(self.input_text):\n","        self.encoder_input_data[i, t, self.input_token_index[char]] = 1.\n","      self.encoder_input_data[i, t + 1:, self.input_token_index[' ']] = 1.\n","      for t, char in enumerate(self.target_text):\n","        # decoder_target_data is ahead of decoder_input_data by one timestep\n","        self.decoder_input_data[i, t, self.target_token_index[char]] = 1.\n","        if t > 0:\n","            # decoder_target_data will be ahead by one timestep\n","            # and will not include the start character.\n","            self.decoder_target_data[i, t - 1, self.target_token_index[char]] = 1.\n","      self.decoder_input_data[i, t + 1:, self.target_token_index[' ']] = 1.\n","      self.decoder_target_data[i, t:, self.target_token_index[' ']] = 1.\n","      self.reverse_input_char_index = dict(\n","        (i, char) for char, i in self.input_token_index.items())\n","\n","      self.reverse_target_char_index = dict(\n","        (i, char) for char, i in self.target_token_index.items())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uRegBrcWi7ad","colab_type":"text"},"source":["> Download and parse the text file "]},{"cell_type":"code","metadata":{"id":"B6-Xye5FGMqB","colab_type":"code","colab":{}},"source":["data_path = 'fra.txt'\n","dataFrEn = datasetFrEg(data_path);"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4S9ppEySLtJO","colab_type":"code","colab":{}},"source":["dataFrEn.loadData();"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o0rcHuoBi_mt","colab_type":"text"},"source":["> Print some global information of the data set"]},{"cell_type":"code","metadata":{"id":"-4eLvLDzLu7-","colab_type":"code","colab":{}},"source":["dataFrEn.printInfo()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LnrAa-K-jI8_","colab_type":"text"},"source":["> Print some sentences "]},{"cell_type":"code","metadata":{"id":"VYiU0SoGLw8t","colab_type":"code","colab":{}},"source":["IdL = [1, 3, 5, 13, 20];\n","\n","for Id in IdL:  \n","  dataFrEn.printSample(Id)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b3utcJIrjNdB","colab_type":"text"},"source":["> As for the previous IMDB data set, the words need to be vectorized into tokens"]},{"cell_type":"code","metadata":{"id":"ep7VHG-vL7qz","colab_type":"code","colab":{}},"source":["dataFrEn.vectorizeData();"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ovc0I0nTJVrv","colab_type":"text"},"source":["## 6.b - Build the translation model \n","> In this section you will build the translation model composed of an encoder and decoder both relaying on an LSTM layer with latent_dim cells."]},{"cell_type":"code","metadata":{"id":"hm71P9IMFvJn","colab_type":"code","colab":{}},"source":["batch_size = 64;  \n","epochs = 10; \n","latent_dim = 256;\n","\n","\n","# Define an input sequence and process it.\n","encoder_inputs = Input(shape=(None, dataFrEn.num_encoder_tokens))\n","\n","encoder = LSTM(latent_dim, return_state=True)\n","encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n","\n","# We discard `encoder_outputs` and only keep the states.\n","encoder_states = [state_h, state_c]\n","\n","\n","# Set up the decoder, using `encoder_states` as initial state.\n","decoder_inputs = Input(shape=(None, dataFrEn.num_decoder_tokens))\n","# We set up our decoder to return full output sequences,\n","# and to return internal states as well. We don't use the\n","# return states in the training model, but we will use them in inference.\n","\n","# Create a LSTM layer for the decoder with latent_dim while activating return_sequences and return_state\n","decoder_lstm = # TO DO \n","decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n","                                     initial_state=encoder_states)\n","\n","#TO DO add 1 output dense layer \n","# what is the number of outputs \n","decoder_dense = # TO DO \n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","# Define the model that will turn\n","# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","# Run training\n","model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","print(model.summary())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DEvFe5AjqGR7","colab_type":"text"},"source":["> Perform the training of the model"]},{"cell_type":"code","metadata":{"id":"2B13Y0luPB0n","colab_type":"code","colab":{}},"source":["model.fit([dataFrEn.encoder_input_data, dataFrEn.decoder_input_data], dataFrEn.decoder_target_data,\n","          batch_size=batch_size,\n","          epochs=epochs,\n","          validation_split=0.2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9HQRjIxDJPaQ","colab_type":"code","colab":{}},"source":["# Next: inference mode (sampling).\n","# Here's the drill:\n","# 1) encode input and retrieve initial decoder state\n","# 2) run one step of decoder with this initial state\n","# and a \"start of sequence\" token as target.\n","# Output will be the next target token\n","# 3) Repeat with the current target token and current states\n","\n","# Define the encoder model \n","encoder_model = Model(encoder_inputs, encoder_states)\n","\n","\n","decoder_state_input_h = Input(shape=(latent_dim,))\n","decoder_state_input_c = Input(shape=(latent_dim,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","decoder_outputs, state_h, state_c = decoder_lstm(\n","    decoder_inputs, initial_state=decoder_states_inputs)\n","decoder_states = [state_h, state_c]\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","# Define the decoder model \n","decoder_model = Model(\n","    [decoder_inputs] + decoder_states_inputs,\n","    [decoder_outputs] + decoder_states)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J61wB2vu-GyS","colab_type":"text"},"source":["## 6.c - Test the translation model \n","> Here you can feed the decode_sequence function by sentence to translate from encoder_input_data and print the translated sentence. \n","\n","> you can increate the number of epochs to 100, to get a more accurate model. "]},{"cell_type":"code","metadata":{"id":"wmiV8r91RiGx","colab_type":"code","colab":{}},"source":["# Reverse-lookup token index to decode sequences back to\n","# something readable.\n","\n","def decode_sequence(input_seq):\n","    # Encode the input as state vectors.\n","    states_value = encoder_model.predict(input_seq)\n","\n","    # Generate empty target sequence of length 1.\n","    target_seq = np.zeros((1, 1, dataFrEn.num_decoder_tokens))\n","    # Populate the first character of target sequence with the start character.\n","    target_seq[0, 0, dataFrEn.target_token_index['\\t']] = 1.\n","\n","    # Sampling loop for a batch of sequences\n","    # (to simplify, here we assume a batch of size 1).\n","    stop_condition = False\n","    decoded_sentence = ''\n","    while not stop_condition:\n","        output_tokens, h, c = decoder_model.predict(\n","            [target_seq] + states_value)\n","\n","        # Sample a token\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_char = dataFrEn.reverse_target_char_index[sampled_token_index]\n","        decoded_sentence += sampled_char\n","\n","        # Exit condition: either hit max length\n","        # or find stop character.\n","        if (sampled_char == '\\n' or\n","           len(decoded_sentence) > dataFrEn.max_decoder_seq_length):\n","            stop_condition = True\n","\n","        # Update the target sequence (of length 1).\n","        target_seq = np.zeros((1, 1, dataFrEn.num_decoder_tokens))\n","        target_seq[0, 0, sampled_token_index] = 1.\n","\n","        # Update states\n","        states_value = [h, c]\n","\n","    return decoded_sentence\n","\n","\n","for seq_index in range(100):\n","    # Take one sequence (part of the training set) from encoder_input_data[i:i+1]\n","    # for trying out decoding with decode_sequence.\n","\n","    # TODO \n","\n","    print('-')\n","    print('Input sentence:', dataFrEn.input_texts[seq_index])\n","    print('Decoded sentence:', decoded_sentence)"],"execution_count":0,"outputs":[]}]}