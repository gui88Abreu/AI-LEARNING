{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"TP5_Transfert_Learning.ipynb","provenance":[{"file_id":"1RT7xAUPQUwgPwPapMKCmPgIdAhl76ISx","timestamp":1585862270398}],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"x9VUagMroNkj","colab_type":"text"},"source":["# **Machine Learning Lab 4EII - IA course**\n","***Lab 5 - Transfer Leearning with Keras***"]},{"cell_type":"markdown","metadata":{"id":"RdNqp46loNkm","colab_type":"text"},"source":["## 1- **Introduction**\n","\n","This lab aims at introducing two important concepts of **transfer learning** in Machine Learning (ML) which are **features extraction** and **fine tuning**.  \n","\n","In this lab you will learn to:\n","* Perform features extraction and classification from a pre-trained deep neural network\n","* Perform fine tuning of a pre-trained deep neural network\n","* Apply these two concepts to food classification problem\n","\t"]},{"cell_type":"markdown","metadata":{"id":"kb0Ckwt51NSu","colab_type":"text"},"source":["## 2- **Definition of transfer learning**\n","\n","Transfer learning is the process of:\n","\n","1.   Taking a network pre-trained on a dataset\n","2.   Use this pre-trained network to recognize image/object categories it was not trained on\n","\n","\n"," "]},{"cell_type":"markdown","metadata":{"id":"lZN_NFuMo9AU","colab_type":"text"},"source":["> In which situations we use **transfer learning** ?"]},{"cell_type":"markdown","metadata":{"id":"2jr0hvobptE1","colab_type":"text"},"source":["The idea behind transfer learning is to take a network trained on a huge dataset like ImageNet ( [ImageNet](http://www.image-net.org/) is a dataset of more then a million of images with 1000 outputs/classes) and use it for another classification of regression problem. \n","\n","In general, there are two types of transfer learning in the context of deep learning:\n","\n","1. Transfer learning via feature extraction\n","2. Transfer learning via fine-tuning\n","\n","In this Lab, you will study thses two concepts of transfer learning based on VGG16 Network trained on ImageNet dataset for image classification. The architecture of VGG16 is illustrated in this Figure. Please refer to this figure in this Lab.  \n","\n","![Architecture of VGG16](https://drive.google.com/uc?id=1cfhg1AmBhj6K0-fdvdKQzQrZqOMgmz27)\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Oc7KsfZWoNks","colab_type":"text"},"source":["## 3- **Features extraction**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LHOn5QhATNph","colab_type":"text"},"source":["In this section you will see how to use features extraction from a pre-trained deep neural network trained on ImageNet dataset to perform classification of images in two classes: Food or Non-food."]},{"cell_type":"markdown","metadata":{"id":"zFcUNh2fR5Fu","colab_type":"text"},"source":["## 3.a - Dataset preparation\n","\n","The first step consists in dataset preparation. We use here a [Food-5K dataset](https://www.epfl.ch/labs/mmspg/downloads/food-image-datasets/) provided by MMSPG - EPFL. This dataset includes 5K images (different from ImageNet images) where 1K images are used for validation, 1K for testing and 3K for training. The images belong to two classes:\n","\n","1 - Food (2500 images)\n","\n","2 - Non-food (2500 images)  "]},{"cell_type":"markdown","metadata":{"id":"D-B28E5VT2uU","colab_type":"text"},"source":["Now you have to configure your working directory and download the dataset: \n","\n","1 - Create a folder called TP5-IA \n","\n","2 - Upload the provided dataset in your working directory. "]},{"cell_type":"code","metadata":{"id":"WhMSvY-OgYk9","colab_type":"code","colab":{}},"source":["import os\n","dirPath = '/content/TP5-IA/'\n","if not os.path.exists(dirPath):\n","  os.makedirs(dirPath)\n","os.chdir(dirPath)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v-myX4UD2eiG","colab_type":"code","colab":{}},"source":["base_path = 'Food-5K-dataset' \n","\n","!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1tigIJiCg7nDsoOEg97ark_yWQEakAKKU' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1tigIJiCg7nDsoOEg97ark_yWQEakAKKU\" -O dataset-20200503T154145Z-001.zip && rm -rf /tmp/cookies.txt\n","!unzip dataset-20200503T154145Z-001.zip \n","%rm 'dataset-20200503T154145Z-001.zip'\n","os.rename('dataset', base_path) "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ELU3Zd8w0gA5","colab_type":"text"},"source":["You can browse the downloaded directory '/content/Food-5K-dataset' to see how the files and images are organized in the dataset. Keras provides some powerful functions to efficiently manage a dataset organized in this way. "]},{"cell_type":"markdown","metadata":{"id":"ioaHUQp4iin6","colab_type":"text"},"source":["You can use this method to display some images of this dataset\n"]},{"cell_type":"code","metadata":{"id":"ltYhczu_ig_L","colab_type":"code","colab":{}},"source":["def displayImage (input_image):\n","  import cv2\n","  from google.colab.patches import cv2_imshow\n","\n","  image = cv2.imread(input_image)\n","  output = image.copy()\n","  cv2_imshow(output)\n","  cv2.waitKey(0)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ys3pVN1uixfD","colab_type":"code","colab":{}},"source":["image = 'Food-5K-dataset/training/food/1_119.jpg'\n","#image = 'Food-5K-dataset/training/non_food/0_100.jpg'\n","displayImage(image)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C7kzj_DETbof","colab_type":"text"},"source":["The following Config class initialises the paths of different sets."]},{"cell_type":"code","metadata":{"id":"jcNOgG4E4z3Z","colab_type":"code","colab":{}},"source":["# Initialisation class \n","from imutils import paths\n","class Config: \n","# initialize the path to the *original* input directory of images\n","  def __init__(self, base_path):\n","    \n","#    self.ORIG_INPUT_DATASET = \"Food-5K\"\n","\n","# initialize the base path to the *new* directory that will contain\n","# our images after computing the training and testing split\n","    self.BASE_PATH = base_path;\n","\n","# define the names of the training, testing, and validation\n","# directories\n","    self.TRAIN = \"training\"\n","    self.TEST = \"evaluation\"\n","    self.VAL = \"validation\"\n","\n","# initialize the list of class label names\n","    self.CLASSES = [];\n","    \n","    if(base_path == 'Food-5K-dataset'):\n","      self.CLASSES = [\"non_food\", \"food\"]\n","    \n","    if(base_path == 'Food-11-dataset'):\n","      self.CLASSES = [\"Bread\", \"Dairy product\", \"Dessert\", \"Egg\", \"Fried food\", \n","                      \"Meat\", \"Noodles/Pasta\", \"Rice\", \"Seafood\", \"Soup\", \n","                      \"Vegetable/Fruit\"]\n","\n","# set the batch size\n","    self.BATCH_SIZE = 32\n","\n","# initialize the label encoder file path and the output directory to\n","# where the extracted features (in CSV file format) will be stored\n","    self.LE_PATH = os.path.sep.join([\"output\", \"le.cpickle\"])\n","    self.BASE_CSV_PATH = \"output\"\n","\n","# set the path to the serialized model after training\n","    self.MODEL_PATH = os.path.sep.join([\"output\", \"model.cpickle\"])\n","\n","# define the path to the output training history plots\n","    self.UNFROZEN_PLOT_PATH = os.path.sep.join([\"output\", \"unfrozen.png\"])\n","    self.WARMUP_PLOT_PATH = os.path.sep.join([\"output\", \"warmup.png\"])\n","\n","    self.trainPath = os.path.sep.join([self.BASE_PATH, self.TRAIN])\n","    self.valPath = os.path.sep.join([self.BASE_PATH, self.VAL])\n","    self.testPath = os.path.sep.join([self.BASE_PATH, self.TEST])\n","    self.totalTrain = len(list(paths.list_images(self.trainPath)))\n","    self.totalVal = len(list(paths.list_images(self.valPath)))\n","    self.totalTest = len(list(paths.list_images(self.testPath)))\n","  \n","  def print_info(self):\n","    \n","# determine the total number of image paths in training, validation,\n","# and testing directories\n","    \n","    print('Size of training set : ',  self.totalTrain)\n","    print('Size of validation set : ',  self.totalVal)\n","    print('Size of testing set : ',  self.totalTest)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sELr7HMXT1qk","colab_type":"text"},"source":["Create an instance of the *Config* class for Food-5K-dataset and print some information with *print_info* method:\n"]},{"cell_type":"code","metadata":{"id":"pfqvcy-aWA7f","colab_type":"code","colab":{}},"source":["base_path = 'Food-5K-dataset' \n","#TO DO "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MMPBrCiOEURC","colab_type":"text"},"source":["## 3.b - Features extraction\n","\n","The idea behind features extraction is to use a pre-trained network to extracted features that will be used as input of a simple machine learning algorithms like SVM, Random Forest or LogisticRegression. In fact, features extracted from a deep neural network trained on a huge dataset can be useful for many other classification problems. As pre-trained network, we can use here several models trained on ImageNet data with million of images and 1000 classes such as VGG16, ResNet, Inception, Xception, etc. \n","\n","In this Lab, you will use VGG16, but you can also test other networks. The weights of the pre-trained VGG16 network can be downloaded from Keras library. You can notice that we are interested by features from VGG16 and not the output classes. So, we download the VGG16 Network without the last classification output layer (without dense layers in green in the VGG-16 figure). You can print the network configuration.  "]},{"cell_type":"code","metadata":{"id":"d5txSYTZLKoC","colab_type":"code","colab":{}},"source":["from tensorflow.keras.applications import VGG16\n","modelVGG16 = VGG16(weights=\"imagenet\", include_top=False) # include_top = False: exclude the last dense layers \n","print(modelVGG16.summary())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"edDFOM2TLuZ0","colab_type":"text"},"source":["Now, you have to build a new class called *FeaturesExtraction* that extracts features using the VGG16 model on Food-5K dataset images. The extracted features and the corresponding labels will be saved in lists: (trainX, trainY), (testX, testY), (valX, valY) containing features and labels of the three sets training, testing and validation of the dataset. You will forward propagate the dataset images in the VGG16 network to extract features.  "]},{"cell_type":"code","metadata":{"id":"JJKg61-OThUK","colab_type":"code","colab":{}},"source":["# USAGE\n","# python extract_features.py\n","\n","# import the necessary packages\n","from sklearn.preprocessing import LabelEncoder\n","\n","from tensorflow.keras.applications import imagenet_utils\n","from tensorflow.keras.preprocessing.image import img_to_array\n","from tensorflow.keras.preprocessing.image import load_img\n","\n","\n","import numpy as np\n","import pickle\n","import random\n","\n","\n","class FeaturesExtraction:\n","  \n","  def __init__(self, model, config):\n","    \n","    self.model = model; \n","    self.config = config;\n","\n","    self.reset();\n","\n","  \n","  def reset (self):\n","    self.trainX = []\n","    self.trainY = []\n","    \n","    self.testX = []\n","    self.testY = []    \n","    \n","    self.valX = []\n","    self.valY = []\n","    self.le = None\n","    \n","  def extractFeatures(self):\n","    self.reset();   \n","    for (split, x, y) in ((self.config.TRAIN, self.trainX, self.trainY), (self.config.TEST,  self.testX, self.testY), (self.config.VAL, self.valX, self.valY)):\n","      print(\"[INFO] processing '{} split'...\".format(split))\n","      p = os.path.sep.join([self.config.BASE_PATH, split])\n","      imagePaths = list(paths.list_images(p))\n","      print(p)\n","      \n","\n","\t    # randomly shuffle the image paths and then extract the class\n","\t    # labels from the file paths\n","      random.shuffle(imagePaths)\n","      labels = [p.split(os.path.sep)[-2] for p in imagePaths]\n","      if self.le is None:\n","        self.le = LabelEncoder()\n","        self.le.fit(labels)\n","\n","      for (b, i) in enumerate(range(0, len(imagePaths), self.config.BATCH_SIZE)):\n","\t\t    # extract the batch of images and labels, then initialize the\n","\t\t    # list of actual images that will be passed through the network\n","\t\t    # for feature extraction\n","        print(\"[INFO] processing batch {}/{}\".format(b + 1, int(np.ceil(len(imagePaths) / float(self.config.BATCH_SIZE)))))\n","        batchPaths = imagePaths[i:i + self.config.BATCH_SIZE]\n","        batchLabels = self.le.transform(labels[i:i + self.config.BATCH_SIZE])\n","        batchImages = []\n","        \n","\t\t    # loop over the images and labels in the current batch\n","        for imagePath in batchPaths:\n","\t\t\t    # load the input image using the Keras helper utility\n","\t\t\t    # while ensuring the image is resized to 224x224 pixels\n","          image = load_img(imagePath, target_size=(224, 224))\n","          image = img_to_array(image)\n","\n","\t\t\t    # preprocess the image by (1) expanding the dimensions and\n","\t\t\t    # (2) subtracting the mean RGB pixel intensity from the\n","\t\t\t    # ImageNet dataset\n","          image = np.expand_dims(image, axis=0)\n","          image = imagenet_utils.preprocess_input(image)\n","\n","\t\t\t    # add the image to the batch\n","        \n","          batchImages.append(image)\n","\n","\t\t      # pass the images through the network and use the outputs as\n","\t\t      # our actual features, then reshape the features into a\n","\t\t      # flattened volume\n","        batchImages = np.vstack(batchImages)\n","        \n","        #TO DO \n","        \n","        #Prediction batchImages\n","\n","        #reshape the output from (see the output size of VGG-16) to 1D vector \n","\n","        \n","        for k in range(0, len(batchLabels)):  \n","          x.append(features[k])\n","          y.append(batchLabels[k])\n","\n","\n","  \n"," "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WkCX00THNdC5","colab_type":"text"},"source":["Create an instance of the FeaturesExtraction."]},{"cell_type":"code","metadata":{"id":"egJx9KkhUHOs","colab_type":"code","colab":{}},"source":["#TO DO"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FOzlyNjd-Y4T","colab_type":"text"},"source":["Call the features extraction class. "]},{"cell_type":"code","metadata":{"id":"1S7OqVh9QMwc","colab_type":"code","colab":{}},"source":["#TO DO "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RIJLkDweQZIn","colab_type":"text"},"source":["## 3.c - Simple Machine Learning model\n","\n","In this section you will create a simple SVM machine learning model then train and test it with features and labels of Food-5K images extracted from the VGG16 Network. You can use standard ML algorithms you have seen in Lab 1 : Random Forest, SVM (linear kernal) or LogisticRegression (solver='lbfgs'). SVM could be long for training."]},{"cell_type":"code","metadata":{"id":"ZegnmqbuReiX","colab_type":"code","colab":{}},"source":["from sklearn.linear_model import LogisticRegression\n","#TO DO"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fJfVX6ZH4liU","colab_type":"text"},"source":["Perform evaluation of the model"]},{"cell_type":"code","metadata":{"id":"o2D5MAOc4zAY","colab_type":"code","colab":{}},"source":["from sklearn.metrics import classification_report\n","#TO DO\n","\n","print(classification_report(self.testY, preds, target_names=self.le.classes_))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F9ZihhHmbAvF","colab_type":"text"},"source":["## 4- **Fine Tuning**\n","\n","In this section you will perform fine tuning of a pre-trained model on ImageNet to solve a more challenging classification problem of Food-11 dataset. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"Zcb6_xyRFP-q","colab_type":"text"},"source":["## 4.a - Dataset preparation\n","\n","The [Food-11](https://www.epfl.ch/labs/mmspg/downloads/food-image-datasets/) dataset consists of 16643 images partitioned into three sets: training, testing and validation. These images belong to 11 major food categories: \n","\n","1 - Bread (1724 images)\n","\n","2 - Dairy product (721 images)\n","\n","3 - Dessert (2,500 images)\n","\n","4 - Egg (1,648 images)\n","\n","5 - Fried food (1,461images)\n","\n","6 - Meat (2,206 images)\n","\n","7 - Noodles/pasta (734 images)\n","\n","8 - Rice (472 images)\n","\n","9 - Seafood (1,505 images)\n","\n","10 - Soup (2,500 images)\n","\n","11 - Vegetable/fruit (1,172 images)\n","\n","You need first to run the following blocks to create the configuration class of Food-11 dataset and download the dataset on your working directory."]},{"cell_type":"code","metadata":{"id":"s7bbub26FUB-","colab_type":"code","colab":{}},"source":["dirPath = '/content/TP5-IA/'\n","if not os.path.exists(dirPath):\n","  os.makedirs(dirPath)\n","os.chdir(dirPath)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mCVtTqxnFXKn","colab_type":"code","colab":{}},"source":["base_path = 'Food-11-dataset' \n","\n","!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1-lt3_fgg5kcnxcCTPYlPdGup95Aa9iCF' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1-lt3_fgg5kcnxcCTPYlPdGup95Aa9iCF\" -O dataset.zip && rm -rf /tmp/cookies.txt\n","!unzip dataset.zip \n","%rm 'dataset.zip'\n","%rm -r '__MACOSX'\n","os.rename('dataset', base_path)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n4P1ax68DNo5","colab_type":"text"},"source":["Create an instance of the Config class for Food-11-dataset:"]},{"cell_type":"code","metadata":{"id":"YnZ7ENgIEABD","colab_type":"code","colab":{}},"source":["base_path = 'Food-11-dataset' \n","#TO DO"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"COjYMeIVSr-M","colab_type":"text"},"source":["You can browse the downloaded directory '/content/Food-51-dataset' to see how the files and images are organized in the dataset. You can also display some samples of the dataset."]},{"cell_type":"code","metadata":{"id":"oA7jgb_8ZJSF","colab_type":"code","colab":{}},"source":["image = 'Food-11-dataset/training/Meat/5_0.jpg'\n","#image = 'Food-11-dataset/training/Soup/9_10.jpg'\n","#image = 'Food-11-dataset/training/Dessert/2_1.jpg'\n","displayImage(image)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YfbmuWBxZJb0","colab_type":"text"},"source":["In this section, you will create data generator for train, validation and test sets. For the training set, the generator will perform some data augmentation such as rotation, zoom, etc. This data augmentation is important for better training and the network will be more robust and accurate regarding those operations."]},{"cell_type":"code","metadata":{"id":"vgUGBSwYY_jM","colab_type":"code","colab":{}},"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# initialize the training data augmentation object\n","trainAug = ImageDataGenerator(\n","\trotation_range=30,\n","\tzoom_range=0.15,\n","\twidth_shift_range=0.2,\n","\theight_shift_range=0.2,\n","\tshear_range=0.15,\n","\thorizontal_flip=True,\n","\tfill_mode=\"nearest\")\n","\n","# initialize the validation/testing data augmentation object (which\n","# we'll be adding mean subtraction to)\n","valAug = ImageDataGenerator()\n","\n","# define the ImageNet mean subtraction (in RGB order) and set the\n","# the mean subtraction value for each of the data augmentation\n","# objects\n","mean = np.array([123.68, 116.779, 103.939], dtype=\"float32\")\n","trainAug.mean = mean\n","valAug.mean = mean\n","\n","# initialize the training generator\n","trainGen = trainAug.flow_from_directory(\n","\tconfigFood11.trainPath,\n","\tclass_mode=\"categorical\",\n","\ttarget_size=(224, 224),\n","\tcolor_mode=\"rgb\",\n","\tshuffle=True,\n","\tbatch_size=configFood11.BATCH_SIZE)\n","\n","# initialize the validation generator\n","valGen = valAug.flow_from_directory(\n","\tconfigFood11.valPath,\n","\tclass_mode=\"categorical\",\n","\ttarget_size=(224, 224),\n","\tcolor_mode=\"rgb\",\n","\tshuffle=False,\n","\tbatch_size=configFood11.BATCH_SIZE)\n","\n","# initialize the testing generator\n","testGen = valAug.flow_from_directory(\n","\tconfigFood11.testPath,\n","\tclass_mode=\"categorical\",\n","\ttarget_size=(224, 224),\n","\tcolor_mode=\"rgb\",\n","\tshuffle=False,\n","\tbatch_size=configFood11.BATCH_SIZE)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_G-6LWNTY1Xd","colab_type":"text"},"source":["## 4.b - Fine-tuning classification model \n","\n","In this section you will build the classification network based on VGG16. The idea is to use the head of VGG16 with additional layers to adapt the output to this problem for food classification. You should add one Flatten layer, one dense layer of size 512 (relu), dropout layer (0.5) and the output layer with softmax activation function.  \n","\n","The concept of fine tuning consists in three mains steps:\n","\n","1 - Build the model based on the pre-trained network (here VGG16) as the head of the network and add fully connected layers to adpt the network to our classification problem. \n","\n","2 - Freeze the head layers from VGG16 and train the model with updating only new added layers (over 50 epochs). \n","\n","3 - Fine tune the model with updating the four last layers of VGG-16 and new dense layers on few epochs (20). \n","\n","Let'us perform these three steps"]},{"cell_type":"markdown","metadata":{"id":"PzLYqltNuhnu","colab_type":"text"},"source":["**1 - Build the model**"]},{"cell_type":"code","metadata":{"id":"F4IuX47nY4w_","colab_type":"code","colab":{}},"source":["from tensorflow.keras.layers import Input, Dropout, Flatten, Dense\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.applications import VGG16\n","\n","# load the VGG16 network, ensuring the head FC layer sets are left\n","# off\n","baseModel = VGG16(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n","\n","# construct the head of the model that will be placed on top of the\n","# the base model\n","\n","headModel = baseModel.output\n","\n","# TO DO\n","\n","model = Model(inputs=baseModel.input, outputs=headModel)\n","print(model.summary())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KN36SHQlfNbr","colab_type":"text"},"source":["**2 - Freeze the head layers from VGG and train the model**\n","\n","\n","This code enable to freeze the layers of VGG16 network. So, you will perform training only on the new layers."]},{"cell_type":"code","metadata":{"id":"jlfLvYREfQt7","colab_type":"code","colab":{}},"source":["for layer in baseModel.layers:\n","\tlayer.trainable = False\n","\n","for layer in baseModel.layers:\n","\tprint(\"{}: {}\".format(layer, layer.trainable))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ECTFY_mXY7b5","colab_type":"text"},"source":["Train the model "]},{"cell_type":"code","metadata":{"id":"oXn2xlSVgfvR","colab_type":"code","colab":{}},"source":["from tensorflow.keras.optimizers import SGD\n","\n","# compile our model (this needs to be done after our setting our\n","# layers to being non-trainable\n","print(\"[INFO] compiling model...\")\n","opt = SGD(lr=1e-4, momentum=0.9)\n","model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n","\tmetrics=[\"accuracy\"])\n","# train the head of the network for a few epochs (all other layers\n","# are frozen) -- this will allow the new FC layers to start to become\n","# initialized with actual \"learned\" values versus pure random\n","print(\"[INFO] training head...\")\n","H = model.fit_generator(\n","\ttrainGen,\n","\tsteps_per_epoch=config.totalTrain // config.BATCH_SIZE,\n","\tvalidation_data=valGen,\n","\tvalidation_steps=config.totalVal // config.BATCH_SIZE,\n","\tepochs=50)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GQBdW-PMym06","colab_type":"text"},"source":["If it is too long to do the training, you can upload the provided weights. "]},{"cell_type":"code","metadata":{"id":"vIYTPkZjomlm","colab_type":"code","colab":{}},"source":["!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1-BHSwPB-cPty7K7BeQA_7-PTc_IXMybG' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1-BHSwPB-cPty7K7BeQA_7-PTc_IXMybG\" -O model_train.h5 && rm -rf /tmp/cookies.txt \n","\n","model.load_weights(\"model_train.h5\")\n","print(\"Weights loaded from disk\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g37VMo5xEJoh","colab_type":"code","colab":{}},"source":["# reset the testing generator and evaluate the network after\n","# fine-tuning just the network head\n","from sklearn.metrics import classification_report\n","print(\"[INFO] evaluating after fine-tuning network head...\")\n","testGen.reset()\n","predIdxs = model.predict_generator(testGen,\n","\tsteps=(config.totalTest // config.BATCH_SIZE) + 1)\n","predIdxs = np.argmax(predIdxs, axis=1)\n","print(classification_report(testGen.classes, predIdxs,\n","\ttarget_names=testGen.class_indices.keys()))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0DJibRlRu9A5","colab_type":"text"},"source":["**3 - Fine tune the model**\n","\n","Unfreeze the last four layers of VGG-16 network (Layers Conv 5-1, Conv 5-2, Conv 5-3 and pooling in Figure 1)"]},{"cell_type":"code","metadata":{"id":"oL5lyKW1vIV9","colab_type":"code","colab":{}},"source":["# now that the head FC layers have been trained/initialized, lets\n","# unfreeze the final set of CONV layers and make them trainable\n","\n","#TO DO \n","\n","# loop over the layers in the model and show which ones are trainable\n","# or not\n","for layer in baseModel.layers:\n","\tprint(\"{}: {}\".format(layer, layer.trainable))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P_Xx2gfjvYBF","colab_type":"text"},"source":["Perform fine tuning of the network "]},{"cell_type":"code","metadata":{"id":"FwPA286xouFR","colab_type":"code","colab":{}},"source":["# reset our data generators\n","trainGen.reset()\n","valGen.reset()\n","\n","\n","# for the changes to the model to take affect we need to recompile\n","# the model, this time using SGD with a *very* small learning rate\n","print(\"[INFO] re-compiling model...\")\n","opt = SGD(lr=1e-4, momentum=0.9)\n","model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n","\tmetrics=[\"accuracy\"])\n","\n","# train the model again, this time fine-tuning *both* the final set\n","# of CONV layers along with our set of FC layers\n","H = model.fit_generator(\n","\ttrainGen,\n","\tsteps_per_epoch=config.totalTrain // config.BATCH_SIZE,\n","\tvalidation_data=valGen,\n","\tvalidation_steps=config.totalVal // config.BATCH_SIZE,\n","\tepochs=20)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TCjRYdEOyTJy","colab_type":"text"},"source":["If it is too long to do the training you can upload the saved weights after this fine-tuning. "]},{"cell_type":"code","metadata":{"id":"_dEZr80fzGBL","colab_type":"code","colab":{}},"source":["!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1-D-Y_ds9284h7i5J1892JLX30V_sIWaN' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1-D-Y_ds9284h7i5J1892JLX30V_sIWaN\" -O model_fine_tune.h5 && rm -rf /tmp/cookies.txt\n","\n","model.load_weights(\"model_fine_tune.h5\")\n","print(\"Weights loaded from disk\")\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yZpehZWlvZdT","colab_type":"text"},"source":["Test the network "]},{"cell_type":"code","metadata":{"id":"Ap6alV7ZteHR","colab_type":"code","colab":{}},"source":["# reset the testing generator and then use our trained model to\n","# make predictions on the data\n","print(\"[INFO] evaluating after fine-tuning network...\")\n","\n","testGen.reset()\n","predIdxs = model.predict_generator(testGen,\n","\tsteps=(config.totalTest // config.BATCH_SIZE) + 1)\n","predIdxs = np.argmax(predIdxs, axis=1)\n","print(classification_report(testGen.classes, predIdxs,\n","\ttarget_names=testGen.class_indices.keys()))\n","\n","# serialize the model to disk\n","if not os.path.exists(config.MODEL_PATH):\n","  os.makedirs(config.MODEL_PATH)\n","\n","print(\"[INFO] serializing network...\")\n","model.save(config.MODEL_PATH)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"svMcMgvhZgmq","colab_type":"text"},"source":["Load an image from the dataset and predict it:"]},{"cell_type":"code","metadata":{"id":"1QzOEn1jZqYi","colab_type":"code","colab":{}},"source":["def predict (input_image, config):\n","  from tensorflow.keras.models import load_model\n","  import numpy as np\n","  import imutils\n","  import cv2\n","  from google.colab.patches import cv2_imshow\n","\n","  image = cv2.imread(input_image)\n","  output = image.copy()\n","  output = imutils.resize(output, width=400)\n","# our model was trained on RGB ordered images but OpenCV represents\n","# images in BGR order, so swap the channels, and then resize to\n","# 224x224 (the input dimensions for VGG16)\n","  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","  image = cv2.resize(image, (224, 224))\n","# convert the image to a floating point data type and perform mean\n","# subtraction\n","  image = image.astype(\"float32\")\n","  mean = np.array([123.68, 116.779, 103.939][::-1], dtype=\"float32\")\n","  image -= mean\n","\n","\n","# load the trained model from disk\n","  print(\"[INFO] loading model...\")\n","  model = load_model(config.MODEL_PATH)\n","# pass the image through the network to obtain our predictions\n","  preds = model.predict(np.expand_dims(image, axis=0))[0]\n","  i = np.argmax(preds)\n","  label = config.CLASSES[i]\n","# draw the prediction on the output image\n","  text = \"{}: {:.2f}%\".format(label, preds[i] * 100)\n","  cv2.putText(output, text, (3, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n","\t  (0, 255, 0), 2)\n","# show the output image\n","  cv2_imshow(output)\n","  cv2.waitKey(0)\n","\n","def predict (input_image, config):\n","  from tensorflow.keras.models import load_model\n","  import numpy as np\n","  import imutils\n","  import cv2\n","  from google.colab.patches import cv2_imshow\n","\n","  image = cv2.imread(input_image)\n","  output = image.copy()\n","  output = imutils.resize(output, width=400)\n","# our model was trained on RGB ordered images but OpenCV represents\n","# images in BGR order, so swap the channels, and then resize to\n","# 224x224 (the input dimensions for VGG16)\n","  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","  image = cv2.resize(image, (224, 224))\n","# convert the image to a floating point data type and perform mean\n","# subtraction\n","  image = image.astype(\"float32\")\n","  mean = np.array([123.68, 116.779, 103.939][::-1], dtype=\"float32\")\n","  image -= mean\n","\n","\n","# load the trained model from disk\n","  print(\"[INFO] loading model...\")\n","  model = load_model(config.MODEL_PATH)\n","# pass the image through the network to obtain our predictions\n","  preds = model.predict(np.expand_dims(image, axis=0))[0]\n","  i = np.argmax(preds)\n","  label = config.CLASSES[i]\n","# draw the prediction on the output image\n","  text = \"{}: {:.2f}%\".format(label, preds[i] * 100)\n","  cv2.putText(output, text, (3, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n","\t  (0, 255, 0), 2)\n","# show the output image\n","  cv2_imshow(output)\n","  cv2.waitKey(0)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rqEdwqrlyMKa","colab_type":"text"},"source":["You can predict the label of an image"]},{"cell_type":"code","metadata":{"id":"v2vYYZLEcKhU","colab_type":"code","colab":{}},"source":["image = 'Food-11-dataset/evaluation/Meat/5_1.jpg'\n","predict(image, config)"],"execution_count":0,"outputs":[]}]}